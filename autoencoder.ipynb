{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4368115,"sourceType":"datasetVersion","datasetId":1152384},{"sourceId":8099754,"sourceType":"datasetVersion","datasetId":4783074}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T12:11:17.220820Z","iopub.execute_input":"2024-04-12T12:11:17.221706Z","iopub.status.idle":"2024-04-12T12:11:18.835706Z","shell.execute_reply.started":"2024-04-12T12:11:17.221672Z","shell.execute_reply":"2024-04-12T12:11:18.834956Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df3 = pd.read_csv('/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv')\ndf3","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:11:21.140554Z","iopub.execute_input":"2024-04-12T12:11:21.141549Z","iopub.status.idle":"2024-04-12T12:11:21.300511Z","shell.execute_reply.started":"2024-04-12T12:11:21.141496Z","shell.execute_reply":"2024-04-12T12:11:21.299527Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                       index                     id  \\\n0      5.74948705591165E+017  5.74948705591165E+017   \n1      5.71917888690393E+017  5.71917888690393E+017   \n2      3.90255841338601E+017  3.90255841338601E+017   \n3      5.68208850655916E+017  5.68208850655916E+017   \n4      5.75596338802373E+017  5.75596338802373E+017   \n...                      ...                    ...   \n16846  5.75606766236475E+017  5.75606766236475E+017   \n16847  5.72333822886326E+017  5.72333822886326E+017   \n16848  5.72326950057845E+017  5.72326950057845E+017   \n16849  5.74799612642357E+017  5.74799612642357E+017   \n16850  5.68826121153684E+017  5.68826121153684E+017   \n\n                                                    Text Annotation  oh_label  \n0      @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n1      @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n2      RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n3      @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n4                                 #mkr No No No No No No       none       0.0  \n...                                                  ...        ...       ...  \n16846  Feeling so sorry for the girls, they should be...       none       0.0  \n16847  #MKR 'pretty good dishes we're happy with' - O...       none       0.0  \n16848  RT @colonelkickhead: Deconstructed lemon tart!...       none       0.0  \n16849  @versacezaynx @nyazpolitics @greenlinerzjm You...       none       0.0  \n16850  And before you protest that you're *not* mad, ...       none       0.0  \n\n[16851 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Annotation</th>\n      <th>oh_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.74948705591165E+017</td>\n      <td>5.74948705591165E+017</td>\n      <td>@halalflaws @biebervalue @greenlinerzjm I read...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.71917888690393E+017</td>\n      <td>5.71917888690393E+017</td>\n      <td>@ShreyaBafna3 Now you idiots claim that people...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.90255841338601E+017</td>\n      <td>3.90255841338601E+017</td>\n      <td>RT @Mooseoftorment Call me sexist, but when I ...</td>\n      <td>sexism</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.68208850655916E+017</td>\n      <td>5.68208850655916E+017</td>\n      <td>@g0ssipsquirrelx Wrong, ISIS follows the examp...</td>\n      <td>racism</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.75596338802373E+017</td>\n      <td>5.75596338802373E+017</td>\n      <td>#mkr No No No No No No</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16846</th>\n      <td>5.75606766236475E+017</td>\n      <td>5.75606766236475E+017</td>\n      <td>Feeling so sorry for the girls, they should be...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16847</th>\n      <td>5.72333822886326E+017</td>\n      <td>5.72333822886326E+017</td>\n      <td>#MKR 'pretty good dishes we're happy with' - O...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16848</th>\n      <td>5.72326950057845E+017</td>\n      <td>5.72326950057845E+017</td>\n      <td>RT @colonelkickhead: Deconstructed lemon tart!...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16849</th>\n      <td>5.74799612642357E+017</td>\n      <td>5.74799612642357E+017</td>\n      <td>@versacezaynx @nyazpolitics @greenlinerzjm You...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16850</th>\n      <td>5.68826121153684E+017</td>\n      <td>5.68826121153684E+017</td>\n      <td>And before you protest that you're *not* mad, ...</td>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16851 rows Ã— 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df3['Text'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-12T11:51:59.839705Z","iopub.execute_input":"2024-04-12T11:51:59.840528Z","iopub.status.idle":"2024-04-12T11:51:59.848547Z","shell.execute_reply.started":"2024-04-12T11:51:59.840493Z","shell.execute_reply":"2024-04-12T11:51:59.847415Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'@halalflaws @biebervalue @greenlinerzjm I read them in context.No change in meaning. The history of Islamic slavery. https://t.co/xWJzpSodGj'"},"metadata":{}}]},{"cell_type":"code","source":"df3.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:11:25.446521Z","iopub.execute_input":"2024-04-12T12:11:25.447498Z","iopub.status.idle":"2024-04-12T12:11:25.466720Z","shell.execute_reply.started":"2024-04-12T12:11:25.447452Z","shell.execute_reply":"2024-04-12T12:11:25.465634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Sample DataFrame\n\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the 'Category' column\ndf3['Encoded_Category'] = label_encoder.fit_transform(df3['Annotation'])\nencoding_map = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n\nprint(\"Encoded values and their corresponding categories:\")\nprint(encoding_map)\n# df3","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:11:29.758048Z","iopub.execute_input":"2024-04-12T12:11:29.758795Z","iopub.status.idle":"2024-04-12T12:11:31.615117Z","shell.execute_reply.started":"2024-04-12T12:11:29.758760Z","shell.execute_reply":"2024-04-12T12:11:31.614092Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Encoded values and their corresponding categories:\n{'none': 0, 'racism': 1, 'sexism': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\n\n# Sample text data and labels\ntexts = df3['Text']  # List of texts\nlabels = df3['Encoded_Category']\ntexts","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:11:58.210007Z","iopub.execute_input":"2024-04-12T12:11:58.211201Z","iopub.status.idle":"2024-04-12T12:11:58.221256Z","shell.execute_reply.started":"2024-04-12T12:11:58.211166Z","shell.execute_reply":"2024-04-12T12:11:58.220068Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0        @halalflaws @biebervalue @greenlinerzjm I read...\n1        @ShreyaBafna3 Now you idiots claim that people...\n2        RT @Mooseoftorment Call me sexist, but when I ...\n3        @g0ssipsquirrelx Wrong, ISIS follows the examp...\n4                                   #mkr No No No No No No\n                               ...                        \n16846    Feeling so sorry for the girls, they should be...\n16847    #MKR 'pretty good dishes we're happy with' - O...\n16848    RT @colonelkickhead: Deconstructed lemon tart!...\n16849    @versacezaynx @nyazpolitics @greenlinerzjm You...\n16850    And before you protest that you're *not* mad, ...\nName: Text, Length: 16848, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Sample text data and labels\ntexts = list(df3['Text'])  # List of texts\nlabels = list(df3['Encoded_Category'])  # List of corresponding labels (0 or 1 for binary classification)\n\n# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:12:03.273402Z","iopub.execute_input":"2024-04-12T12:12:03.273806Z","iopub.status.idle":"2024-04-12T12:12:03.302380Z","shell.execute_reply.started":"2024-04-12T12:12:03.273776Z","shell.execute_reply":"2024-04-12T12:12:03.301302Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_texts)\n\n# Padding the data samples to a maximum review length\n\nvect_train = pad_sequences(tokenizer.texts_to_sequences(train_texts))\nvect_test = pad_sequences(tokenizer.texts_to_sequences(val_texts))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:28:27.056486Z","iopub.execute_input":"2024-04-12T12:28:27.056929Z","iopub.status.idle":"2024-04-12T12:28:27.979465Z","shell.execute_reply.started":"2024-04-12T12:28:27.056899Z","shell.execute_reply":"2024-04-12T12:28:27.978224Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tokenizer.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect_train,vect_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:13:15.693771Z","iopub.execute_input":"2024-04-12T12:13:15.694208Z","iopub.status.idle":"2024-04-12T12:13:15.703065Z","shell.execute_reply.started":"2024-04-12T12:13:15.694178Z","shell.execute_reply":"2024-04-12T12:13:15.701718Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(array([[    0,     0,     0, ...,   447,     1,  6530],\n        [    0,     0,     0, ...,   112,    20,   554],\n        [    0,     0,     0, ...,   652,     4,  9676],\n        ...,\n        [    0,     0,     0, ..., 22781,     6, 22782],\n        [    0,     0,     0, ...,  1011,    18,     3],\n        [    0,     0,     0, ..., 22786,    19, 22787]], dtype=int32),\n (13478, 34))"},"metadata":{}}]},{"cell_type":"code","source":"vect_train_tensor = torch.tensor(vect_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:13:28.359364Z","iopub.execute_input":"2024-04-12T12:13:28.359773Z","iopub.status.idle":"2024-04-12T12:13:28.412486Z","shell.execute_reply.started":"2024-04-12T12:13:28.359741Z","shell.execute_reply":"2024-04-12T12:13:28.411491Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"vect_train_tensor = vect_train_tensor.float()\nvect_train_tensor","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:13:30.139301Z","iopub.execute_input":"2024-04-12T12:13:30.140509Z","iopub.status.idle":"2024-04-12T12:13:30.251733Z","shell.execute_reply.started":"2024-04-12T12:13:30.140464Z","shell.execute_reply":"2024-04-12T12:13:30.250652Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4700e+02, 1.0000e+00,\n         6.5300e+03],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1200e+02, 2.0000e+01,\n         5.5400e+02],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.5200e+02, 4.0000e+00,\n         9.6760e+03],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2781e+04, 6.0000e+00,\n         2.2782e+04],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0110e+03, 1.8000e+01,\n         3.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2786e+04, 1.9000e+01,\n         2.2787e+04]])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass Seq2SeqAutoencoder(nn.Module):\n    def __init__(self, num_words, embed_dim, hidden_dim):\n        super(Seq2SeqAutoencoder, self).__init__()\n        self.embedding = nn.Embedding(num_words, embed_dim)\n        self.encoder_lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True)\n        self.decoder_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True)\n        self.output_layer = nn.Linear(hidden_dim * 2, num_words)\n        \n    def forward(self, inputs):\n        embedded = self.embedding(inputs)\n        encoder_outputs, (hidden, cell) = self.encoder_lstm(embedded)\n        \n        # Repeat the last hidden and cell state for the decoder LSTM\n        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1).unsqueeze(0)\n        cell = torch.cat([cell[-2], cell[-1]], dim=1).unsqueeze(0)\n        \n        # Decoder input is a repeated version of the last encoder hidden state\n        decoder_input = hidden.repeat(inputs.size(0), 1, 1)\n        \n        # Decode sequence using decoder LSTM\n        decoder_outputs, _ = self.decoder_lstm(decoder_input)\n        \n        # Output layer\n        outputs = self.output_layer(decoder_outputs)\n        \n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:13:32.535504Z","iopub.execute_input":"2024-04-12T12:13:32.536215Z","iopub.status.idle":"2024-04-12T12:13:32.546686Z","shell.execute_reply.started":"2024-04-12T12:13:32.536183Z","shell.execute_reply":"2024-04-12T12:13:32.545705Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vect_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T11:57:03.022923Z","iopub.execute_input":"2024-04-12T11:57:03.023301Z","iopub.status.idle":"2024-04-12T11:57:03.029312Z","shell.execute_reply.started":"2024-04-12T11:57:03.023275Z","shell.execute_reply":"2024-04-12T11:57:03.028259Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(13478, 34)"},"metadata":{}}]},{"cell_type":"code","source":"CUDA_LAUNCH_BLOCKING = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-12T11:52:58.891223Z","iopub.execute_input":"2024-04-12T11:52:58.892107Z","iopub.status.idle":"2024-04-12T11:52:58.896091Z","shell.execute_reply.started":"2024-04-12T11:52:58.892055Z","shell.execute_reply":"2024-04-12T11:52:58.895085Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\n\nif cuda_available:\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"CUDA is available with {num_gpus} GPU(s) available.\")\nelse:\n    print(\"CUDA is not available. Please ensure that GPU support is enabled.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T11:49:07.878185Z","iopub.execute_input":"2024-04-12T11:49:07.878513Z","iopub.status.idle":"2024-04-12T11:49:07.966507Z","shell.execute_reply.started":"2024-04-12T11:49:07.878489Z","shell.execute_reply":"2024-04-12T11:49:07.965574Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"CUDA is available with 2 GPU(s) available.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(device)\n# Define hyperparameters\nnum_words = 25000  # Number of words in the vocabulary\nembed_dim = 50   # Embedding dimension\nhidden_dim = 128  # Hidden dimension of LSTM layers\nbatch_size = 16\nnum_epochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Create model instance and move it to GPU\nmodel = Seq2SeqAutoencoder(num_words, embed_dim, hidden_dim)\nmodel.to(device)\nprint(device)\n\n# Define optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Convert input data to PyTorch tensors and move to GPU\npad_seqs_tensor = torch.tensor(vect_train, dtype=torch.long).to(device)\ntarget_tensor = torch.tensor(vect_train, dtype=torch.long).to(device)\n\n# Total number of samples in the dataset\ntotal_samples = len(pad_seqs_tensor)\n\n# Training loop\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n    \n    # Mini-batch training\n    for i in range(0, total_samples, batch_size):\n        # Get mini-batch\n        inputs = pad_seqs_tensor[i:i+batch_size]\n        targets = target_tensor[i:i+batch_size]\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape outputs and targets for loss calculation\n        outputs = outputs.view(-1, num_words)\n        targets = targets.view(-1)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Accumulate the total loss\n        total_loss += loss.item()\n    \n    # Calculate average loss\n    average_loss = total_loss / (total_samples // batch_size)\n    \n    print('Epoch [{}/{}], Average Loss: {:.4f}'.format(epoch+1, num_epochs, average_loss))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:15:12.958788Z","iopub.execute_input":"2024-04-12T12:15:12.959662Z","iopub.status.idle":"2024-04-12T12:23:58.657012Z","shell.execute_reply.started":"2024-04-12T12:15:12.959619Z","shell.execute_reply":"2024-04-12T12:23:58.655816Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"cuda\nEpoch [1/50], Average Loss: 3.9067\nEpoch [2/50], Average Loss: 3.5259\nEpoch [3/50], Average Loss: 3.2780\nEpoch [4/50], Average Loss: 3.0356\nEpoch [5/50], Average Loss: 2.7917\nEpoch [6/50], Average Loss: 2.5629\nEpoch [7/50], Average Loss: 2.3563\nEpoch [8/50], Average Loss: 2.1844\nEpoch [9/50], Average Loss: 2.0398\nEpoch [10/50], Average Loss: 1.9205\nEpoch [11/50], Average Loss: 1.8176\nEpoch [12/50], Average Loss: 1.7225\nEpoch [13/50], Average Loss: 1.6361\nEpoch [14/50], Average Loss: 1.5544\nEpoch [15/50], Average Loss: 1.4801\nEpoch [16/50], Average Loss: 1.4107\nEpoch [17/50], Average Loss: 1.3476\nEpoch [18/50], Average Loss: 1.2919\nEpoch [19/50], Average Loss: 1.2423\nEpoch [20/50], Average Loss: 1.1888\nEpoch [21/50], Average Loss: 1.1389\nEpoch [22/50], Average Loss: 1.0947\nEpoch [23/50], Average Loss: 1.0534\nEpoch [24/50], Average Loss: 1.0148\nEpoch [25/50], Average Loss: 0.9769\nEpoch [26/50], Average Loss: 0.9415\nEpoch [27/50], Average Loss: 0.9078\nEpoch [28/50], Average Loss: 0.8797\nEpoch [29/50], Average Loss: 0.8461\nEpoch [30/50], Average Loss: 0.8162\nEpoch [31/50], Average Loss: 0.7884\nEpoch [32/50], Average Loss: 0.7668\nEpoch [33/50], Average Loss: 0.7463\nEpoch [34/50], Average Loss: 0.7205\nEpoch [35/50], Average Loss: 0.6982\nEpoch [36/50], Average Loss: 0.6772\nEpoch [37/50], Average Loss: 0.6597\nEpoch [38/50], Average Loss: 0.6405\nEpoch [39/50], Average Loss: 0.6214\nEpoch [40/50], Average Loss: 0.6036\nEpoch [41/50], Average Loss: 0.5861\nEpoch [42/50], Average Loss: 0.5712\nEpoch [43/50], Average Loss: 0.5593\nEpoch [44/50], Average Loss: 0.5443\nEpoch [45/50], Average Loss: 0.5284\nEpoch [46/50], Average Loss: 0.5171\nEpoch [47/50], Average Loss: 0.5020\nEpoch [48/50], Average Loss: 0.4844\nEpoch [49/50], Average Loss: 0.4742\nEpoch [50/50], Average Loss: 0.4659\n","output_type":"stream"}]},{"cell_type":"code","source":"weights_path = 'model_weights_new.pth'\n\n# Save the model weights\ntorch.save(model.state_dict(), weights_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:24:36.983297Z","iopub.execute_input":"2024-04-12T12:24:36.983734Z","iopub.status.idle":"2024-04-12T12:24:37.047882Z","shell.execute_reply.started":"2024-04-12T12:24:36.983701Z","shell.execute_reply":"2024-04-12T12:24:37.046659Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\n# Load the trained autoencoder model\n# autoencoder = Seq2SeqAutoencoder(num_words, embed_dim, hidden_dim)\n# autoencoder.load_state_dict(torch.load('model_weights.pth'))\nmodel.to(\"cpu\")\nmodel.eval()  # Set the model to evaluation mode\n\n# Extract the encoder part\nencoder = model.encoder_lstm\nencoder.to(\"cpu\")\n# Convert text data to PyTorch tensors\ntexts_tensor = torch.tensor(vect_test, dtype=torch.long)\n\n# Pass text data through the encoder\nwith torch.no_grad():\n    # Convert the input tensor to the same data type as the weights of the LSTM layer\n    texts_tensor = texts_tensor.to(torch.float)  # Convert to float\n    texts_tensor.to(\"cpu\")\n    embeddings, _ = encoder(texts_tensor)\n\n# Convert embeddings to numpy arrays\nembeddings_np = embeddings.numpy()\n\n# Now you have embeddings for your text data, you can use them for downstream tasks\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:30:53.927879Z","iopub.execute_input":"2024-04-12T12:30:53.928817Z","iopub.status.idle":"2024-04-12T12:30:54.192700Z","shell.execute_reply.started":"2024-04-12T12:30:53.928778Z","shell.execute_reply":"2024-04-12T12:30:54.191584Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"embeddings_np.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:31:03.533525Z","iopub.execute_input":"2024-04-12T12:31:03.533950Z","iopub.status.idle":"2024-04-12T12:31:03.541092Z","shell.execute_reply.started":"2024-04-12T12:31:03.533920Z","shell.execute_reply":"2024-04-12T12:31:03.539777Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(3370, 256)"},"metadata":{}}]},{"cell_type":"code","source":"class ANNClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CustomLoss(nn.Module):\n    def __init__(self, lambda_reg=0.01, alpha=0.25, gamma=2.0, beta=0.5):\n        super(CustomLoss, self).__init__()\n        self.lambda_reg = lambda_reg\n        self.alpha = alpha\n        self.gamma = gamma\n        self.beta = beta\n        self.bce_loss = nn.CrossEntropyLoss()\n\n    def forward(self, y_true, y_pred, model):\n        # Binary Cross-Entropy Loss\n        bce_loss = self.bce_loss(y_pred, y_true)\n\n        # Regularization Loss (L2 regularization)\n        l2_reg = torch.tensor(0., requires_grad=True)\n        for param in model.parameters():\n            l2_reg += torch.norm(param)**2\n        regularization_loss = self.lambda_reg * l2_reg\n\n        # Focal Loss\n        focal_loss = -torch.mean(\n            self.alpha * y_true * (1 - y_pred)**self.gamma * torch.log(y_pred + 1e-8) +\n            (1 - self.alpha) * (1 - y_true) * y_pred**self.gamma * torch.log(1 - y_pred + 1e-8)\n        )\n\n        # Penalized BCE Loss for False Negatives\n        penalized_bce_loss = -torch.mean(\n            self.alpha * y_true * torch.log(y_pred + 1e-8) +\n            (1 - y_true) * torch.log(1 - y_pred + 1e-8) +\n            self.beta * y_true * (1 - y_pred)\n        )\n\n        # Combine the losses\n        total_loss = penalized_bce_loss + regularization_loss + focal_loss\n\n        return total_loss\n\n\n# Example usage\n# Assuming y_true, y_pred, and model are available\nloss_fn = CustomLoss()  # Adjust alpha, beta, and other hyperparameters as needed\n# loss = loss_fn(y_true, y_pred, model)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:36:37.784846Z","iopub.execute_input":"2024-04-12T12:36:37.785783Z","iopub.status.idle":"2024-04-12T12:36:37.796482Z","shell.execute_reply.started":"2024-04-12T12:36:37.785752Z","shell.execute_reply":"2024-04-12T12:36:37.795370Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, lambda_reg=0.01, alpha=0.25, gamma=2.0, beta=0.5):\n        super(CustomLoss, self).__init__()\n        self.lambda_reg = lambda_reg\n        self.alpha = alpha\n        self.gamma = gamma\n        self.beta = beta\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, y_true, y_pred, model):\n        # Binary Cross-Entropy Loss\n        bce_loss = self.bce_loss(y_pred, y_true)\n\n        # Regularization Loss (L2 regularization)\n        l2_reg = torch.tensor(0., requires_grad=True)\n        for param in model.parameters():\n            l2_reg = l2_reg + torch.norm(param)**2  # Accumulate without in-place operation\n        regularization_loss = self.lambda_reg * l2_reg\n\n        # Focal Loss\n        focal_loss = -torch.mean(\n            self.alpha * y_true * (1 - y_pred)**self.gamma * torch.log(y_pred + 1e-8) +\n            (1 - self.alpha) * (1 - y_true) * y_pred**self.gamma * torch.log(1 - y_pred + 1e-8)\n        )\n\n        # Penalized BCE Loss for False Negatives\n        penalized_bce_loss = -torch.mean(\n            self.alpha * y_true * torch.log(y_pred + 1e-8) +\n            (1 - y_true) * torch.log(1 - y_pred + 1e-8) +\n            self.beta * y_true * (1 - y_pred)\n        )\n\n        # Combine the losses\n        total_loss = penalized_bce_loss + regularization_loss + focal_loss\n\n        return total_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\n# Assuming CustomLoss, ANNClassifier, X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor are defined\n\n# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(embeddings, val_labels, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n\n# One-hot encode the targets\nnum_classes = 3  # Adjust based on the number of classes\ny_train_tensor = nn.functional.one_hot(torch.tensor(y_train), num_classes=num_classes).float()\ny_test_tensor = nn.functional.one_hot(torch.tensor(y_test), num_classes=num_classes).float()\n\n# Define hyperparameters\ninput_dim = 256  # Dimension of the embeddings\nhidden_dim = 128\noutput_dim = 3  # Number of output classes\nlearning_rate = 0.001\nnum_epochs = 300\n\n# Create the model instance\nmodel = ANNClassifier(input_dim, hidden_dim, output_dim)\n\n# Define loss function\ncriterion = CustomLoss()\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = model(X_train_tensor)\n\n    # Calculate loss\n    loss = criterion(outputs, y_train_tensor, model)\n\n    # Backward pass and optimization\n    loss.backward()\n    optimizer.step()\n\n    # Print loss every epoch\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs, 1)\n    accuracy = (predicted == y_test_tensor.argmax(dim=1)).sum().item() / len(y_test_tensor)\n    print(f\"Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:38:35.381131Z","iopub.execute_input":"2024-04-12T12:38:35.381541Z","iopub.status.idle":"2024-04-12T12:38:38.325981Z","shell.execute_reply.started":"2024-04-12T12:38:35.381513Z","shell.execute_reply":"2024-04-12T12:38:38.324891Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4143331158.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n/tmp/ipykernel_34/4143331158.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/300], Loss: 9.451236724853516\nEpoch [2/300], Loss: 9.235913276672363\nEpoch [3/300], Loss: 9.011756896972656\nEpoch [4/300], Loss: 8.778804779052734\nEpoch [5/300], Loss: 8.537607192993164\nEpoch [6/300], Loss: 8.2883939743042\nEpoch [7/300], Loss: 8.032262802124023\nEpoch [8/300], Loss: 7.770830154418945\nEpoch [9/300], Loss: 7.5063395500183105\nEpoch [10/300], Loss: 7.242185115814209\nEpoch [11/300], Loss: 6.982159614562988\nEpoch [12/300], Loss: 6.730072021484375\nEpoch [13/300], Loss: 6.489934921264648\nEpoch [14/300], Loss: 6.265532493591309\nEpoch [15/300], Loss: 6.05965518951416\nEpoch [16/300], Loss: 5.874083042144775\nEpoch [17/300], Loss: 5.7096405029296875\nEpoch [18/300], Loss: 5.5660247802734375\nEpoch [19/300], Loss: 5.4422783851623535\nEpoch [20/300], Loss: 5.336816787719727\nEpoch [21/300], Loss: 5.247716426849365\nEpoch [22/300], Loss: 5.172943592071533\nEpoch [23/300], Loss: 5.110447883605957\nEpoch [24/300], Loss: 5.05831241607666\nEpoch [25/300], Loss: 5.0148234367370605\nEpoch [26/300], Loss: 4.978455066680908\nEpoch [27/300], Loss: 4.947925090789795\nEpoch [28/300], Loss: 4.92214822769165\nEpoch [29/300], Loss: 4.900235176086426\nEpoch [30/300], Loss: 4.8814544677734375\nEpoch [31/300], Loss: 4.865196704864502\nEpoch [32/300], Loss: 4.850982666015625\nEpoch [33/300], Loss: 4.838414192199707\nEpoch [34/300], Loss: 4.8271870613098145\nEpoch [35/300], Loss: 4.817047119140625\nEpoch [36/300], Loss: 4.807788372039795\nEpoch [37/300], Loss: 4.799247741699219\nEpoch [38/300], Loss: 4.791295528411865\nEpoch [39/300], Loss: 4.783825874328613\nEpoch [40/300], Loss: 4.77675724029541\nEpoch [41/300], Loss: 4.770025730133057\nEpoch [42/300], Loss: 4.763576984405518\nEpoch [43/300], Loss: 4.757368087768555\nEpoch [44/300], Loss: 4.751368045806885\nEpoch [45/300], Loss: 4.745551109313965\nEpoch [46/300], Loss: 4.739893913269043\nEpoch [47/300], Loss: 4.734381675720215\nEpoch [48/300], Loss: 4.728998184204102\nEpoch [49/300], Loss: 4.723733901977539\nEpoch [50/300], Loss: 4.71858024597168\nEpoch [51/300], Loss: 4.713530540466309\nEpoch [52/300], Loss: 4.708581447601318\nEpoch [53/300], Loss: 4.703728199005127\nEpoch [54/300], Loss: 4.698967933654785\nEpoch [55/300], Loss: 4.694299221038818\nEpoch [56/300], Loss: 4.689717769622803\nEpoch [57/300], Loss: 4.6852216720581055\nEpoch [58/300], Loss: 4.680810928344727\nEpoch [59/300], Loss: 4.676483154296875\nEpoch [60/300], Loss: 4.672236442565918\nEpoch [61/300], Loss: 4.668071269989014\nEpoch [62/300], Loss: 4.663985252380371\nEpoch [63/300], Loss: 4.659976005554199\nEpoch [64/300], Loss: 4.656044006347656\nEpoch [65/300], Loss: 4.652188301086426\nEpoch [66/300], Loss: 4.648406505584717\nEpoch [67/300], Loss: 4.644698619842529\nEpoch [68/300], Loss: 4.6410627365112305\nEpoch [69/300], Loss: 4.6374969482421875\nEpoch [70/300], Loss: 4.634001731872559\nEpoch [71/300], Loss: 4.6305742263793945\nEpoch [72/300], Loss: 4.627214431762695\nEpoch [73/300], Loss: 4.623920440673828\nEpoch [74/300], Loss: 4.62069034576416\nEpoch [75/300], Loss: 4.617522716522217\nEpoch [76/300], Loss: 4.614416599273682\nEpoch [77/300], Loss: 4.611370086669922\nEpoch [78/300], Loss: 4.6083831787109375\nEpoch [79/300], Loss: 4.605453014373779\nEpoch [80/300], Loss: 4.6025776863098145\nEpoch [81/300], Loss: 4.599757194519043\nEpoch [82/300], Loss: 4.596989631652832\nEpoch [83/300], Loss: 4.594274044036865\nEpoch [84/300], Loss: 4.591609477996826\nEpoch [85/300], Loss: 4.588994026184082\nEpoch [86/300], Loss: 4.58642578125\nEpoch [87/300], Loss: 4.583905220031738\nEpoch [88/300], Loss: 4.581429958343506\nEpoch [89/300], Loss: 4.5789995193481445\nEpoch [90/300], Loss: 4.576612949371338\nEpoch [91/300], Loss: 4.574268817901611\nEpoch [92/300], Loss: 4.571966171264648\nEpoch [93/300], Loss: 4.569703102111816\nEpoch [94/300], Loss: 4.567479610443115\nEpoch [95/300], Loss: 4.565293788909912\nEpoch [96/300], Loss: 4.563145160675049\nEpoch [97/300], Loss: 4.561033248901367\nEpoch [98/300], Loss: 4.558956623077393\nEpoch [99/300], Loss: 4.556914806365967\nEpoch [100/300], Loss: 4.554906368255615\nEpoch [101/300], Loss: 4.55293083190918\nEpoch [102/300], Loss: 4.550987243652344\nEpoch [103/300], Loss: 4.549075126647949\nEpoch [104/300], Loss: 4.547192573547363\nEpoch [105/300], Loss: 4.545340538024902\nEpoch [106/300], Loss: 4.543516635894775\nEpoch [107/300], Loss: 4.541721343994141\nEpoch [108/300], Loss: 4.539953231811523\nEpoch [109/300], Loss: 4.538212299346924\nEpoch [110/300], Loss: 4.536498069763184\nEpoch [111/300], Loss: 4.534809112548828\nEpoch [112/300], Loss: 4.533145904541016\nEpoch [113/300], Loss: 4.5315070152282715\nEpoch [114/300], Loss: 4.529892921447754\nEpoch [115/300], Loss: 4.52830171585083\nEpoch [116/300], Loss: 4.526734352111816\nEpoch [117/300], Loss: 4.525188446044922\nEpoch [118/300], Loss: 4.5236663818359375\nEpoch [119/300], Loss: 4.522164821624756\nEpoch [120/300], Loss: 4.520685195922852\nEpoch [121/300], Loss: 4.51922607421875\nEpoch [122/300], Loss: 4.517787933349609\nEpoch [123/300], Loss: 4.516369342803955\nEpoch [124/300], Loss: 4.514970779418945\nEpoch [125/300], Loss: 4.5135908126831055\nEpoch [126/300], Loss: 4.512229919433594\nEpoch [127/300], Loss: 4.510887622833252\nEpoch [128/300], Loss: 4.509564399719238\nEpoch [129/300], Loss: 4.508257865905762\nEpoch [130/300], Loss: 4.506968975067139\nEpoch [131/300], Loss: 4.505697250366211\nEpoch [132/300], Loss: 4.504443168640137\nEpoch [133/300], Loss: 4.503204822540283\nEpoch [134/300], Loss: 4.501982688903809\nEpoch [135/300], Loss: 4.5007758140563965\nEpoch [136/300], Loss: 4.499585151672363\nEpoch [137/300], Loss: 4.498409271240234\nEpoch [138/300], Loss: 4.497248649597168\nEpoch [139/300], Loss: 4.496102333068848\nEpoch [140/300], Loss: 4.49497127532959\nEpoch [141/300], Loss: 4.49385404586792\nEpoch [142/300], Loss: 4.492751121520996\nEpoch [143/300], Loss: 4.491661071777344\nEpoch [144/300], Loss: 4.490585803985596\nEpoch [145/300], Loss: 4.489522933959961\nEpoch [146/300], Loss: 4.488473892211914\nEpoch [147/300], Loss: 4.487436294555664\nEpoch [148/300], Loss: 4.486412048339844\nEpoch [149/300], Loss: 4.485400199890137\nEpoch [150/300], Loss: 4.484400749206543\nEpoch [151/300], Loss: 4.4834136962890625\nEpoch [152/300], Loss: 4.482438087463379\nEpoch [153/300], Loss: 4.48147439956665\nEpoch [154/300], Loss: 4.480521202087402\nEpoch [155/300], Loss: 4.479580402374268\nEpoch [156/300], Loss: 4.478649616241455\nEpoch [157/300], Loss: 4.4777302742004395\nEpoch [158/300], Loss: 4.476821422576904\nEpoch [159/300], Loss: 4.475922584533691\nEpoch [160/300], Loss: 4.475034713745117\nEpoch [161/300], Loss: 4.474157333374023\nEpoch [162/300], Loss: 4.4732890129089355\nEpoch [163/300], Loss: 4.472430229187012\nEpoch [164/300], Loss: 4.47158145904541\nEpoch [165/300], Loss: 4.470742225646973\nEpoch [166/300], Loss: 4.469912528991699\nEpoch [167/300], Loss: 4.46909236907959\nEpoch [168/300], Loss: 4.468279838562012\nEpoch [169/300], Loss: 4.467476844787598\nEpoch [170/300], Loss: 4.4666829109191895\nEpoch [171/300], Loss: 4.4658966064453125\nEpoch [172/300], Loss: 4.465120792388916\nEpoch [173/300], Loss: 4.464352130889893\nEpoch [174/300], Loss: 4.463591575622559\nEpoch [175/300], Loss: 4.462839126586914\nEpoch [176/300], Loss: 4.462095260620117\nEpoch [177/300], Loss: 4.461359024047852\nEpoch [178/300], Loss: 4.460631370544434\nEpoch [179/300], Loss: 4.4599103927612305\nEpoch [180/300], Loss: 4.459197998046875\nEpoch [181/300], Loss: 4.458491802215576\nEpoch [182/300], Loss: 4.457793712615967\nEpoch [183/300], Loss: 4.457101821899414\nEpoch [184/300], Loss: 4.456418037414551\nEpoch [185/300], Loss: 4.455740928649902\nEpoch [186/300], Loss: 4.455070495605469\nEpoch [187/300], Loss: 4.454407691955566\nEpoch [188/300], Loss: 4.4537506103515625\nEpoch [189/300], Loss: 4.453100681304932\nEpoch [190/300], Loss: 4.452456474304199\nEpoch [191/300], Loss: 4.45181941986084\nEpoch [192/300], Loss: 4.451189041137695\nEpoch [193/300], Loss: 4.450565338134766\nEpoch [194/300], Loss: 4.449947357177734\nEpoch [195/300], Loss: 4.44933557510376\nEpoch [196/300], Loss: 4.448729515075684\nEpoch [197/300], Loss: 4.448129653930664\nEpoch [198/300], Loss: 4.447535514831543\nEpoch [199/300], Loss: 4.44694709777832\nEpoch [200/300], Loss: 4.4463653564453125\nEpoch [201/300], Loss: 4.445788383483887\nEpoch [202/300], Loss: 4.445217609405518\nEpoch [203/300], Loss: 4.444652080535889\nEpoch [204/300], Loss: 4.444091796875\nEpoch [205/300], Loss: 4.44353723526001\nEpoch [206/300], Loss: 4.44298791885376\nEpoch [207/300], Loss: 4.44244384765625\nEpoch [208/300], Loss: 4.4419050216674805\nEpoch [209/300], Loss: 4.441371917724609\nEpoch [210/300], Loss: 4.440842151641846\nEpoch [211/300], Loss: 4.440317630767822\nEpoch [212/300], Loss: 4.439798355102539\nEpoch [213/300], Loss: 4.439283847808838\nEpoch [214/300], Loss: 4.438774108886719\nEpoch [215/300], Loss: 4.438268661499023\nEpoch [216/300], Loss: 4.437767028808594\nEpoch [217/300], Loss: 4.4372711181640625\nEpoch [218/300], Loss: 4.436779022216797\nEpoch [219/300], Loss: 4.436291694641113\nEpoch [220/300], Loss: 4.4358086585998535\nEpoch [221/300], Loss: 4.435329437255859\nEpoch [222/300], Loss: 4.434853553771973\nEpoch [223/300], Loss: 4.434383392333984\nEpoch [224/300], Loss: 4.433917045593262\nEpoch [225/300], Loss: 4.4334540367126465\nEpoch [226/300], Loss: 4.432995319366455\nEpoch [227/300], Loss: 4.432540416717529\nEpoch [228/300], Loss: 4.432088851928711\nEpoch [229/300], Loss: 4.431641578674316\nEpoch [230/300], Loss: 4.431199550628662\nEpoch [231/300], Loss: 4.430759906768799\nEpoch [232/300], Loss: 4.430324077606201\nEpoch [233/300], Loss: 4.429891586303711\nEpoch [234/300], Loss: 4.42946195602417\nEpoch [235/300], Loss: 4.429036617279053\nEpoch [236/300], Loss: 4.428615570068359\nEpoch [237/300], Loss: 4.428196907043457\nEpoch [238/300], Loss: 4.427781581878662\nEpoch [239/300], Loss: 4.427369594573975\nEpoch [240/300], Loss: 4.4269609451293945\nEpoch [241/300], Loss: 4.42655611038208\nEpoch [242/300], Loss: 4.426153659820557\nEpoch [243/300], Loss: 4.425754547119141\nEpoch [244/300], Loss: 4.425358772277832\nEpoch [245/300], Loss: 4.424966335296631\nEpoch [246/300], Loss: 4.424576282501221\nEpoch [247/300], Loss: 4.42418909072876\nEpoch [248/300], Loss: 4.423805236816406\nEpoch [249/300], Loss: 4.423423767089844\nEpoch [250/300], Loss: 4.423046588897705\nEpoch [251/300], Loss: 4.422670364379883\nEpoch [252/300], Loss: 4.422297954559326\nEpoch [253/300], Loss: 4.4219279289245605\nEpoch [254/300], Loss: 4.421560287475586\nEpoch [255/300], Loss: 4.421195030212402\nEpoch [256/300], Loss: 4.420832633972168\nEpoch [257/300], Loss: 4.420474052429199\nEpoch [258/300], Loss: 4.420117378234863\nEpoch [259/300], Loss: 4.41976261138916\nEpoch [260/300], Loss: 4.419410705566406\nEpoch [261/300], Loss: 4.419060707092285\nEpoch [262/300], Loss: 4.418713569641113\nEpoch [263/300], Loss: 4.418368816375732\nEpoch [264/300], Loss: 4.418025016784668\nEpoch [265/300], Loss: 4.417685508728027\nEpoch [266/300], Loss: 4.417348861694336\nEpoch [267/300], Loss: 4.417012691497803\nEpoch [268/300], Loss: 4.416678428649902\nEpoch [269/300], Loss: 4.416347503662109\nEpoch [270/300], Loss: 4.416017055511475\nEpoch [271/300], Loss: 4.415689468383789\nEpoch [272/300], Loss: 4.4153642654418945\nEpoch [273/300], Loss: 4.415042877197266\nEpoch [274/300], Loss: 4.4147210121154785\nEpoch [275/300], Loss: 4.414402008056641\nEpoch [276/300], Loss: 4.4140849113464355\nEpoch [277/300], Loss: 4.413769721984863\nEpoch [278/300], Loss: 4.413455963134766\nEpoch [279/300], Loss: 4.413143157958984\nEpoch [280/300], Loss: 4.412832736968994\nEpoch [281/300], Loss: 4.4125261306762695\nEpoch [282/300], Loss: 4.412219047546387\nEpoch [283/300], Loss: 4.4119157791137695\nEpoch [284/300], Loss: 4.4116129875183105\nEpoch [285/300], Loss: 4.411313533782959\nEpoch [286/300], Loss: 4.41101598739624\nEpoch [287/300], Loss: 4.410715103149414\nEpoch [288/300], Loss: 4.410416603088379\nEpoch [289/300], Loss: 4.410120964050293\nEpoch [290/300], Loss: 4.409828186035156\nEpoch [291/300], Loss: 4.409534454345703\nEpoch [292/300], Loss: 4.409244060516357\nEpoch [293/300], Loss: 4.408955097198486\nEpoch [294/300], Loss: 4.40866756439209\nEpoch [295/300], Loss: 4.408379554748535\nEpoch [296/300], Loss: 4.408094882965088\nEpoch [297/300], Loss: 4.407811164855957\nEpoch [298/300], Loss: 4.407526969909668\nEpoch [299/300], Loss: 4.407245635986328\nEpoch [300/300], Loss: 4.406966209411621\nAccuracy: 0.685459940652819\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\n# Define the architecture of the ANN\nclass ANNClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n\n# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(embeddings, val_labels, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# Define hyperparameters\ninput_dim = 256  # Dimension of the embeddings\nhidden_dim = 128\noutput_dim = 3  # Number of output classes\nlearning_rate = 0.001\nnum_epochs = 300\n\n# Create the model instance\nmodel = ANNClassifier(input_dim, hidden_dim, output_dim)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = model(X_train_tensor)\n\n    # Calculate loss\n    loss = criterion(outputs, y_train_tensor)\n\n    # Backward pass and optimization\n    loss.backward()\n    optimizer.step()\n\n    # Print loss every epoch\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_test_tensor)\n    _, predicted = torch.max(outputs, 1)\n    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n    print(f\"Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:41:51.078776Z","iopub.execute_input":"2024-04-12T12:41:51.079848Z","iopub.status.idle":"2024-04-12T12:41:53.557197Z","shell.execute_reply.started":"2024-04-12T12:41:51.079795Z","shell.execute_reply":"2024-04-12T12:41:53.555912Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/983138744.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n/tmp/ipykernel_34/983138744.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/300], Loss: 1.1021819114685059\nEpoch [2/300], Loss: 1.0885121822357178\nEpoch [3/300], Loss: 1.0746015310287476\nEpoch [4/300], Loss: 1.0604547262191772\nEpoch [5/300], Loss: 1.0461019277572632\nEpoch [6/300], Loss: 1.0316061973571777\nEpoch [7/300], Loss: 1.0170124769210815\nEpoch [8/300], Loss: 1.002436637878418\nEpoch [9/300], Loss: 0.9880458116531372\nEpoch [10/300], Loss: 0.9740267395973206\nEpoch [11/300], Loss: 0.9605998992919922\nEpoch [12/300], Loss: 0.9479926824569702\nEpoch [13/300], Loss: 0.9363787174224854\nEpoch [14/300], Loss: 0.9259083271026611\nEpoch [15/300], Loss: 0.9166440963745117\nEpoch [16/300], Loss: 0.9085982441902161\nEpoch [17/300], Loss: 0.9017220735549927\nEpoch [18/300], Loss: 0.8959259986877441\nEpoch [19/300], Loss: 0.8910844922065735\nEpoch [20/300], Loss: 0.8870795965194702\nEpoch [21/300], Loss: 0.8837841749191284\nEpoch [22/300], Loss: 0.88108229637146\nEpoch [23/300], Loss: 0.8788697719573975\nEpoch [24/300], Loss: 0.877059280872345\nEpoch [25/300], Loss: 0.8755753040313721\nEpoch [26/300], Loss: 0.8743547201156616\nEpoch [27/300], Loss: 0.8733460903167725\nEpoch [28/300], Loss: 0.8725100755691528\nEpoch [29/300], Loss: 0.8718134760856628\nEpoch [30/300], Loss: 0.8712296485900879\nEpoch [31/300], Loss: 0.870737612247467\nEpoch [32/300], Loss: 0.8703203797340393\nEpoch [33/300], Loss: 0.8699637651443481\nEpoch [34/300], Loss: 0.8696573376655579\nEpoch [35/300], Loss: 0.8693917393684387\nEpoch [36/300], Loss: 0.8691601753234863\nEpoch [37/300], Loss: 0.8689565062522888\nEpoch [38/300], Loss: 0.8687763214111328\nEpoch [39/300], Loss: 0.8686155080795288\nEpoch [40/300], Loss: 0.8684712052345276\nEpoch [41/300], Loss: 0.8683409094810486\nEpoch [42/300], Loss: 0.8682223558425903\nEpoch [43/300], Loss: 0.8681135177612305\nEpoch [44/300], Loss: 0.8680130243301392\nEpoch [45/300], Loss: 0.8679194450378418\nEpoch [46/300], Loss: 0.8678321838378906\nEpoch [47/300], Loss: 0.8677498698234558\nEpoch [48/300], Loss: 0.8676717877388\nEpoch [49/300], Loss: 0.8675973415374756\nEpoch [50/300], Loss: 0.867526113986969\nEpoch [51/300], Loss: 0.8674573302268982\nEpoch [52/300], Loss: 0.8673908710479736\nEpoch [53/300], Loss: 0.8673263192176819\nEpoch [54/300], Loss: 0.8672634959220886\nEpoch [55/300], Loss: 0.8672019243240356\nEpoch [56/300], Loss: 0.8671416640281677\nEpoch [57/300], Loss: 0.8670819997787476\nEpoch [58/300], Loss: 0.8670229315757751\nEpoch [59/300], Loss: 0.8669642806053162\nEpoch [60/300], Loss: 0.8669059872627258\nEpoch [61/300], Loss: 0.8668477535247803\nEpoch [62/300], Loss: 0.8667894005775452\nEpoch [63/300], Loss: 0.8667312860488892\nEpoch [64/300], Loss: 0.8666728734970093\nEpoch [65/300], Loss: 0.8666141629219055\nEpoch [66/300], Loss: 0.8665552139282227\nEpoch [67/300], Loss: 0.8664958477020264\nEpoch [68/300], Loss: 0.8664361238479614\nEpoch [69/300], Loss: 0.8663764595985413\nEpoch [70/300], Loss: 0.8663159012794495\nEpoch [71/300], Loss: 0.8662547469139099\nEpoch [72/300], Loss: 0.8661931157112122\nEpoch [73/300], Loss: 0.8661307096481323\nEpoch [74/300], Loss: 0.8660675883293152\nEpoch [75/300], Loss: 0.8660035729408264\nEpoch [76/300], Loss: 0.8659385442733765\nEpoch [77/300], Loss: 0.8658720850944519\nEpoch [78/300], Loss: 0.8658046126365662\nEpoch [79/300], Loss: 0.8657358884811401\nEpoch [80/300], Loss: 0.8656656742095947\nEpoch [81/300], Loss: 0.8655943274497986\nEpoch [82/300], Loss: 0.8655217885971069\nEpoch [83/300], Loss: 0.8654478192329407\nEpoch [84/300], Loss: 0.8653723001480103\nEpoch [85/300], Loss: 0.8652952909469604\nEpoch [86/300], Loss: 0.8652162551879883\nEpoch [87/300], Loss: 0.865135669708252\nEpoch [88/300], Loss: 0.8650534152984619\nEpoch [89/300], Loss: 0.8649691343307495\nEpoch [90/300], Loss: 0.8648828268051147\nEpoch [91/300], Loss: 0.8647947311401367\nEpoch [92/300], Loss: 0.8647050857543945\nEpoch [93/300], Loss: 0.8646140694618225\nEpoch [94/300], Loss: 0.8645219802856445\nEpoch [95/300], Loss: 0.8644280433654785\nEpoch [96/300], Loss: 0.8643322587013245\nEpoch [97/300], Loss: 0.8642346262931824\nEpoch [98/300], Loss: 0.8641344904899597\nEpoch [99/300], Loss: 0.8640321493148804\nEpoch [100/300], Loss: 0.8639275431632996\nEpoch [101/300], Loss: 0.863820493221283\nEpoch [102/300], Loss: 0.8637109398841858\nEpoch [103/300], Loss: 0.8635985851287842\nEpoch [104/300], Loss: 0.8634830713272095\nEpoch [105/300], Loss: 0.8633649349212646\nEpoch [106/300], Loss: 0.8632437586784363\nEpoch [107/300], Loss: 0.8631195425987244\nEpoch [108/300], Loss: 0.862991988658905\nEpoch [109/300], Loss: 0.8628613352775574\nEpoch [110/300], Loss: 0.8627273440361023\nEpoch [111/300], Loss: 0.8625898361206055\nEpoch [112/300], Loss: 0.8624492883682251\nEpoch [113/300], Loss: 0.8623049855232239\nEpoch [114/300], Loss: 0.8621559739112854\nEpoch [115/300], Loss: 0.862002432346344\nEpoch [116/300], Loss: 0.8618444800376892\nEpoch [117/300], Loss: 0.8616822957992554\nEpoch [118/300], Loss: 0.8615154027938843\nEpoch [119/300], Loss: 0.8613431453704834\nEpoch [120/300], Loss: 0.8611648678779602\nEpoch [121/300], Loss: 0.8609800934791565\nEpoch [122/300], Loss: 0.8607887029647827\nEpoch [123/300], Loss: 0.8605912923812866\nEpoch [124/300], Loss: 0.8603873252868652\nEpoch [125/300], Loss: 0.8601766228675842\nEpoch [126/300], Loss: 0.8599598407745361\nEpoch [127/300], Loss: 0.8597357869148254\nEpoch [128/300], Loss: 0.8595041632652283\nEpoch [129/300], Loss: 0.8592652678489685\nEpoch [130/300], Loss: 0.8590186834335327\nEpoch [131/300], Loss: 0.8587647676467896\nEpoch [132/300], Loss: 0.858501672744751\nEpoch [133/300], Loss: 0.8582302927970886\nEpoch [134/300], Loss: 0.8579491972923279\nEpoch [135/300], Loss: 0.8576581478118896\nEpoch [136/300], Loss: 0.8573578596115112\nEpoch [137/300], Loss: 0.8570489883422852\nEpoch [138/300], Loss: 0.8567309379577637\nEpoch [139/300], Loss: 0.8564026355743408\nEpoch [140/300], Loss: 0.8560658097267151\nEpoch [141/300], Loss: 0.8557198643684387\nEpoch [142/300], Loss: 0.8553648591041565\nEpoch [143/300], Loss: 0.8550010919570923\nEpoch [144/300], Loss: 0.8546285033226013\nEpoch [145/300], Loss: 0.8542473316192627\nEpoch [146/300], Loss: 0.8538587689399719\nEpoch [147/300], Loss: 0.853462278842926\nEpoch [148/300], Loss: 0.8530585765838623\nEpoch [149/300], Loss: 0.8526471257209778\nEpoch [150/300], Loss: 0.8522276282310486\nEpoch [151/300], Loss: 0.8518001437187195\nEpoch [152/300], Loss: 0.8513650894165039\nEpoch [153/300], Loss: 0.8509233593940735\nEpoch [154/300], Loss: 0.8504736423492432\nEpoch [155/300], Loss: 0.8500155210494995\nEpoch [156/300], Loss: 0.8495502471923828\nEpoch [157/300], Loss: 0.8490779995918274\nEpoch [158/300], Loss: 0.8485984802246094\nEpoch [159/300], Loss: 0.8481117486953735\nEpoch [160/300], Loss: 0.8476194143295288\nEpoch [161/300], Loss: 0.8471215963363647\nEpoch [162/300], Loss: 0.8466190099716187\nEpoch [163/300], Loss: 0.8461111783981323\nEpoch [164/300], Loss: 0.8455979228019714\nEpoch [165/300], Loss: 0.8450793027877808\nEpoch [166/300], Loss: 0.8445544838905334\nEpoch [167/300], Loss: 0.8440262079238892\nEpoch [168/300], Loss: 0.8434940576553345\nEpoch [169/300], Loss: 0.8429577350616455\nEpoch [170/300], Loss: 0.8424166440963745\nEpoch [171/300], Loss: 0.8418710231781006\nEpoch [172/300], Loss: 0.8413205742835999\nEpoch [173/300], Loss: 0.8407663702964783\nEpoch [174/300], Loss: 0.8402085304260254\nEpoch [175/300], Loss: 0.8396463394165039\nEpoch [176/300], Loss: 0.8390795588493347\nEpoch [177/300], Loss: 0.8385093808174133\nEpoch [178/300], Loss: 0.8379347324371338\nEpoch [179/300], Loss: 0.8373562693595886\nEpoch [180/300], Loss: 0.8367740511894226\nEpoch [181/300], Loss: 0.8361877202987671\nEpoch [182/300], Loss: 0.835597574710846\nEpoch [183/300], Loss: 0.835003674030304\nEpoch [184/300], Loss: 0.8344070315361023\nEpoch [185/300], Loss: 0.8338073492050171\nEpoch [186/300], Loss: 0.8332076072692871\nEpoch [187/300], Loss: 0.8326077461242676\nEpoch [188/300], Loss: 0.8320065140724182\nEpoch [189/300], Loss: 0.831405758857727\nEpoch [190/300], Loss: 0.8308053016662598\nEpoch [191/300], Loss: 0.8302052021026611\nEpoch [192/300], Loss: 0.8296077847480774\nEpoch [193/300], Loss: 0.8290098905563354\nEpoch [194/300], Loss: 0.8284136652946472\nEpoch [195/300], Loss: 0.8278175592422485\nEpoch [196/300], Loss: 0.8272231221199036\nEpoch [197/300], Loss: 0.8266294598579407\nEpoch [198/300], Loss: 0.8260374665260315\nEpoch [199/300], Loss: 0.8254472613334656\nEpoch [200/300], Loss: 0.8248589038848877\nEpoch [201/300], Loss: 0.8242745995521545\nEpoch [202/300], Loss: 0.8236938118934631\nEpoch [203/300], Loss: 0.8231157660484314\nEpoch [204/300], Loss: 0.822539746761322\nEpoch [205/300], Loss: 0.8219649791717529\nEpoch [206/300], Loss: 0.8213912844657898\nEpoch [207/300], Loss: 0.8208183646202087\nEpoch [208/300], Loss: 0.8202451467514038\nEpoch [209/300], Loss: 0.8196707963943481\nEpoch [210/300], Loss: 0.8190956711769104\nEpoch [211/300], Loss: 0.8185208439826965\nEpoch [212/300], Loss: 0.8179446458816528\nEpoch [213/300], Loss: 0.8173678517341614\nEpoch [214/300], Loss: 0.8167904615402222\nEpoch [215/300], Loss: 0.8162118196487427\nEpoch [216/300], Loss: 0.8156331777572632\nEpoch [217/300], Loss: 0.8150559663772583\nEpoch [218/300], Loss: 0.8144808411598206\nEpoch [219/300], Loss: 0.8139083981513977\nEpoch [220/300], Loss: 0.8133417963981628\nEpoch [221/300], Loss: 0.8127825260162354\nEpoch [222/300], Loss: 0.8122299313545227\nEpoch [223/300], Loss: 0.8116841912269592\nEpoch [224/300], Loss: 0.8111452460289001\nEpoch [225/300], Loss: 0.81061190366745\nEpoch [226/300], Loss: 0.8100869655609131\nEpoch [227/300], Loss: 0.8095683455467224\nEpoch [228/300], Loss: 0.8090553879737854\nEpoch [229/300], Loss: 0.8085471987724304\nEpoch [230/300], Loss: 0.8080428838729858\nEpoch [231/300], Loss: 0.8075430989265442\nEpoch [232/300], Loss: 0.8070456981658936\nEpoch [233/300], Loss: 0.8065497279167175\nEpoch [234/300], Loss: 0.8060551881790161\nEpoch [235/300], Loss: 0.8055626153945923\nEpoch [236/300], Loss: 0.8050717115402222\nEpoch [237/300], Loss: 0.8045827150344849\nEpoch [238/300], Loss: 0.804095983505249\nEpoch [239/300], Loss: 0.8036130666732788\nEpoch [240/300], Loss: 0.8031370639801025\nEpoch [241/300], Loss: 0.802669882774353\nEpoch [242/300], Loss: 0.8022134900093079\nEpoch [243/300], Loss: 0.801769495010376\nEpoch [244/300], Loss: 0.8013381958007812\nEpoch [245/300], Loss: 0.8009193539619446\nEpoch [246/300], Loss: 0.8005125522613525\nEpoch [247/300], Loss: 0.8001158237457275\nEpoch [248/300], Loss: 0.799728274345398\nEpoch [249/300], Loss: 0.7993471026420593\nEpoch [250/300], Loss: 0.7989727258682251\nEpoch [251/300], Loss: 0.7986038327217102\nEpoch [252/300], Loss: 0.7982387542724609\nEpoch [253/300], Loss: 0.7978752851486206\nEpoch [254/300], Loss: 0.7975144982337952\nEpoch [255/300], Loss: 0.7971531748771667\nEpoch [256/300], Loss: 0.7967900633811951\nEpoch [257/300], Loss: 0.7964231967926025\nEpoch [258/300], Loss: 0.796051561832428\nEpoch [259/300], Loss: 0.7956740260124207\nEpoch [260/300], Loss: 0.7952916026115417\nEpoch [261/300], Loss: 0.7949060201644897\nEpoch [262/300], Loss: 0.7945155501365662\nEpoch [263/300], Loss: 0.7941216230392456\nEpoch [264/300], Loss: 0.7937285304069519\nEpoch [265/300], Loss: 0.7933388352394104\nEpoch [266/300], Loss: 0.7929518222808838\nEpoch [267/300], Loss: 0.7925692200660706\nEpoch [268/300], Loss: 0.7921921610832214\nEpoch [269/300], Loss: 0.7918220162391663\nEpoch [270/300], Loss: 0.7914621829986572\nEpoch [271/300], Loss: 0.7911137938499451\nEpoch [272/300], Loss: 0.7907776236534119\nEpoch [273/300], Loss: 0.7904547452926636\nEpoch [274/300], Loss: 0.7901446223258972\nEpoch [275/300], Loss: 0.7898476123809814\nEpoch [276/300], Loss: 0.7895619869232178\nEpoch [277/300], Loss: 0.7892864346504211\nEpoch [278/300], Loss: 0.7890192866325378\nEpoch [279/300], Loss: 0.7887586951255798\nEpoch [280/300], Loss: 0.7885023355484009\nEpoch [281/300], Loss: 0.788246750831604\nEpoch [282/300], Loss: 0.7879900336265564\nEpoch [283/300], Loss: 0.7877328395843506\nEpoch [284/300], Loss: 0.787474513053894\nEpoch [285/300], Loss: 0.7872145771980286\nEpoch [286/300], Loss: 0.7869553565979004\nEpoch [287/300], Loss: 0.7866986989974976\nEpoch [288/300], Loss: 0.7864454388618469\nEpoch [289/300], Loss: 0.7861957550048828\nEpoch [290/300], Loss: 0.7859508991241455\nEpoch [291/300], Loss: 0.7857109308242798\nEpoch [292/300], Loss: 0.7854754328727722\nEpoch [293/300], Loss: 0.7852451801300049\nEpoch [294/300], Loss: 0.7850170731544495\nEpoch [295/300], Loss: 0.7847880721092224\nEpoch [296/300], Loss: 0.7845562100410461\nEpoch [297/300], Loss: 0.7843202948570251\nEpoch [298/300], Loss: 0.7840808629989624\nEpoch [299/300], Loss: 0.7838399410247803\nEpoch [300/300], Loss: 0.7836012840270996\nAccuracy: 0.6676557863501483\n","output_type":"stream"}]}]}